---
title: "HW 4"
author: "You"
date: "`r Sys.Date()`"
output: html_document
---

1. Install and load package rstanarm

```{r echo = F}
library(tidyverse)
library(rstanarm)
options(mc.cores = parallel::detectCores())
```


2. Read chapters 8 and 9

Then complete the following exercises:

8.8 Comparing lm and stan_glm: Use simulated data to compare least squares estimation to default Bayesian regression:
a. Simulate 100 data points from the model, y = 2 + 3x + error, with predictors x drawn from a uniform distribution from 0 to 20, and with independent errors drawn from the normal distribution with mean 0 and standard deviation 5. Fit the regression of y on x data using lm and stan_glm (using its default settings) and check that the two programs give nearly
identical results.

```{r}
# to get started
errors<-rnorm(100, 0, 5)
x<-runif(100, 0, 20)
# then compute y using y = 2 + 3x + error
mod_dat<-data.frame(y = 2 + 3 * x + errors,
                    x = x)

m0_f<-lm(y~x, data = mod_dat)
m0_b<-stan_glm(y~x, data = mod_dat, refresh = 0)

coef(m0_f)
coef(m0_b)
```

Those results are nearly identical! So it really doesn't matter if we use the frequentist or Bayesian approach, with default priors we'll get indistinguishable results. 

b. Plot the simulated data and the two fitted regression lines.

```{r}
ggplot(mod_dat,
       aes(x = x, y = y)) + 
  geom_point() + 
  geom_abline(intercept = coef(m0_f)[1], slope = coef(m0_f)[2],
              color = "green") + 
  geom_abline(intercept = coef(m0_b)[1], slope = coef(m0_b)[2],
              color = "blue") 
```

The lines overlaps almost completely, so we can't see any distinction here. 

c. Repeat the two steps above, but try to create conditions for your simulation so that lm and
stan_glm give much different results. (OPTIONAL)

Many of you noted that the way to do this is to set a wacky prior! That's right. If we set a strong prior far away from the mean of the data, we can influence the model.

9.3 Uncertainty in the predicted expectation and the forecast: Consider the economy and voting example from Section 7.1. Fit the linear regression model to the data through 2012; these are available in the folder ElectionsEconomy. Make a forecast for the incumbent party’s share of the two-party vote in a future election where economic growth is 2%.
a. Compute the point forecast, the standard deviation of the predicted expectation from (9.1), and the standard deviation of the predicted value from (9.2).

```{r}
dat<-read_delim("./data/hibbs.dat")

m0<-stan_glm(vote ~ growth, data = dat, refresh = 0)

e_vote<-posterior_predict(m0, newdata = data.frame(growth = 2))

```

We see a value of 3.0 for $\beta_1$ and 46.4 for $\beta_0$, suggesting that expected vote share should be $\beta_0 + \beta_1 \times 2 = 52.3$ when growth is 2%. We can get a standard deviation of a prediction by combining our uncertainty in $\beta$ and $\epsilon$: $\sqrt{SE^2_{\beta_0} + SE^2_{\beta_1} + \sigma^2} = \sqrt{1.6^2 + 0.7^2 + 3.9^2} = 4.3$

b. Now compute these using the relevant prediction functions discussed in Section 9.2. Check that you get the same values as in part (a) of this problem

The model generates a predicted vote share centered at `r mean(e_vote)`, with a standard deviation of `r sd(e_vote)` using sample from the posterior predictive distribution. These results look very similar to the results obtained in a). 


9.5 Combining prior information and data: A new job training program is being tested. Based on the successes and failures of previously proposed innovations, your prior distribution on the effect size on log(income) is normal with a mean of −0.02 and a standard deviation of 0.05. You then conduct an experiment which gives an unbiased estimate of the treatment effect of 0.16 with a standard deviation of 0.08. What is the posterior mean and standard deviation of the treatment effect?

Using equations 9.3 and 9.4 from the book: 

```{r}
# posterior mean
(1/0.05^2 * -0.02 + 1/0.08^2 * 0.16) / (1/0.05^2 + 1/0.08^2)
# posterior sd
1/sqrt(1/0.05^2 + 1/0.08^2)
```


9.7 Uniform, weakly informative, and informative priors: Follow the steps of Section 9.5 for a different example, a regression of earnings on height using the data from the folder Earnings. You will need to think what could be an informative prior distribution in this setting. Hint: You will need to manually choose a 1) uniform, 2) weakly informative, and 3) informative prior. Explain your choices. 

```{r}
dat<-read_csv("./data/earnings.csv")

m_uniform<-stan_glm(scale(earn) ~ scale(height), 
                    data = dat,
                    prior = NULL,
                    prior_intercept = NULL)

m_weak<-stan_glm(scale(earn) ~ scale(height), 
                    data = dat,
                    prior = normal(0, 10),
                    prior_intercept = normal(0,10))

m_strong<-stan_glm(scale(earn) ~ scale(height), 
                    data = dat,
                    prior = normal(0, 1),
                    prior_intercept = normal(0,1))

coef(m_uniform)
coef(m_weak)
coef(m_strong)
```

First, we center earnings and height at their means and transform onto an *z* scale, easing interpretation and the setting of priors. We estimate one model with a uniform distribution, one model with weakly informative priors $\beta \sim N(0, 10)$ and one with more informative priors $\beta \sim N(0, 1)$

With stronger priors, we shrink the intercept estimate a bit toward zero, but the basic findings of the model are unchanged. Prior selection doesn't make too much of a difference in this context. The estimated slope term of 0.3 is well covered by even our informative Normal(0,1) prior, so it's not unreasonable to obtain a 0.3 posterior estimate. 