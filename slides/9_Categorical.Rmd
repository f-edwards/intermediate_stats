---
title: "Categorical data and regression"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, echo = FALSE, message = FALSE}
rm(list=ls())
library(tidyverse)
library(broom)
select<-dplyr::select
set.seed(1)

options(xtable.comment = FALSE)
theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

## Categorical data

Categorical data falls into a fixed set of categories. It may be \textit{unordered}, meaning that there is no inherent ranking of categories, or it may be \textit{ordered}. Ordered categorical data has an explicit hierarchical ranking of values. 

## Categorical data, examples

Are these variables ordered or unordered? \pause

- Candidate choice in a primary election \pause
- Zip code for people choosing a place to move \pause
- Cause of death \pause
- Opinions on a political issua on a thermometer / Likert scale (e.g. Strongly oppose, oppose, neutral, support, strongly support) \pause
- Graduate program to attend \pause
- Ranking of graduate program 

## Visualzing categorical data

```{r}
data(iris)
```

Crosstabs are often the best

```{r}
table(iris$Species)
```

## Visualzing categorical data (cont.)

```{r size = "scriptsize"}
iris%>%group_by(Species)%>%summarise(Petal.Length = mean(Petal.Length))
```

## Visualizing categorical data - frequency barplots

```{r size = "tiny"}
fe<-read_csv("./data/fe_1_25_19.csv")
ggplot(fe, aes(x = `Cause of death`)) + 
  geom_bar() + 
  coord_flip()
```

## Visualizing categorical data, facets

```{r size = "tiny"}
ggplot(iris, aes(x = Petal.Length)) + 
  geom_histogram() + 
  facet_wrap(~Species)
```

## Visualizing categorical data, facets

```{r size = "tiny"}
ggplot(iris, aes(x = Petal.Length)) + 
  geom_histogram() + 
  facet_wrap(~Species, ncol=1)
```

## Visualizing categorical data, facets

```{r size = "tiny"}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width)) + 
  geom_point() + 
  facet_wrap(~Species)
```

## Visualizing categorical data, color

```{r size = "tiny"}
ggplot(iris, aes(x = Petal.Length, y = Petal.Width, 
                 color = Species)) + 
  geom_point() 
```

## Predicting categorical outcomes, logit approach

We can use logistic regression to predict the likelihood that a categorical outcome is equal to one value relative to all others. For K categories, we need to estimate K models with this approach. 

```{r size = "tiny"}
m_setosa<-glm(Species=="setosa" ~
                Petal.Width + Petal.Length, 
              data = iris, 
              family = "binomial")

m_versicolor<-glm(Species=="versicolor" ~
                Petal.Width + Petal.Length, 
              data = iris, 
              family = "binomial")

m_virginica<-glm(Species=="virginica" ~
                Petal.Width + Petal.Length, 
              data = iris, 
              family = "binomial")

```

## Check the model results

```{r size = "tiny"}
library(broom)
tidy(m_setosa)
```

## Can we predict species?

```{r size = "tiny", fig.height = 3}
preds_setosa<-data.frame(
  pred = predict(m_setosa, 
                 type = "response"),
  species = "setosa")
preds_versicolor<-data.frame(
  pred = predict(m_versicolor, type = "response"),
                             species = "versicolor")
preds_virginica<-data.frame(
  pred = predict(m_virginica, type = "response"),
  species = "virginica")
preds_out<-bind_rows(preds_setosa, preds_versicolor, preds_virginica)

ggplot(preds_out, aes(x = pred)) + geom_histogram() + facet_wrap(~species)
```

## Any problems with this approach?

```{r size = "tiny"}
p_total<-preds_setosa$pred + preds_versicolor$pred + preds_virginica$pred
summary(p_total)
```

```{r fig.height=5}
qplot(p_total)
```

## Problems with this approach

1. We discard information by reducing outcome to binary \pause
2. Because we are separately estimating models, nothing constrains $\sum{p}=1$ \pause
3. This can lead to conflicting classifications 

## An alternative for unordered categorical data

Multinomial logistic regression is a GLM that models the log odds of a categorical outcome as a function of a linear combination of a set of predictors. \pause

In R, we use the nnet package and the multinom function. 

## Multinomial logistic regression: basics

For a categorical outcome with $K$ categories, estimate $K - 1$ models where 1,2,3 stand in for membership in group 1, 2, 3:

$$log \frac{Pr(y_i =1)}{Pr(y_i=K)} =  \beta x_i$$
$$log \frac{Pr(y_i =2)}{Pr(y_i=K)} =  \beta x_i$$
$$ \cdots$$
$$log \frac{Pr(y_i =K-1)}{Pr(y_i=K)} =  \beta x_i$$

Key assumtion: Independence of irrelevant alternatives. Odds of choice do not depend on the presence or absence of other alternatives (i.e. car vs bus or car vs red bus vs blue bus)

## Implementation

1. Choose a reference category. This is arbitrary, but changes the interpretation. Remember that we're modeling the log odds of membership in one group relative to another.

2. Estimate a model

3. Interpret results

## Implementation 

```{r size = "tiny", echo = FALSE}
df<-read_table("https://data.princeton.edu/wws509/datasets/mobility.dat")
df_out<-list()
for(i in 1:nrow(df)){
  df_temp<-df[i,]
  df_out[[i]]<-df_temp[rep(seq_len(nrow(df_temp)), df_temp$n), ]
}
df<-bind_rows(df_out)%>%
  select(-X1, -n)
```

```{r}
lapply(df, unique)
## reference category for outcome
df<-df%>%
  mutate(sonOccup = factor(sonOccup, 
                           levels = c("unskilled", "farm", "skilled", "professional")))
```

## Let's predict social mobility

```{r}
library(nnet)
library(broom)
m1<-multinom(sonOccup ~ fatherOccup + black, data = df)
```

## Let's interpret this

Same approach as a logit model

1. Log odds ($\beta$) of option 1 vs reference
2. Odds ratio ($e^\beta$) of option 1 vs reference
3. Probability of outcome \pause

However, now we effectively have coefficients for K-1 models to look at.

## Interpreting the model (Log odds and odds ratio)
```{r, size = "tiny"}
tidy(m1)%>%select(y.level, term, estimate, std.error)%>%mutate(OR = exp(estimate))
```

## Interpreting the model (probability)

```{r size = "tiny"}
preds<-as.data.frame(predict(m1, type = "probs"))
df%>%bind_cols(preds)%>%select(-nonintact, -sonOccup)%>%distinct()
```

## Comparing models

```{r size = "tiny"}
m2<-multinom(sonOccup ~ fatherOccup + black + nonintact, data = df)
BIC(m1)
BIC(m2)
```

## Going further

- For ordered categorical variables, consider using ordinal regression methods. 
- polr in the MASS package estimates proportional odds logistic regression models for ordered categorical varibles


