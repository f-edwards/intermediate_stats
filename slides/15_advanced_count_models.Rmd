---
title: "Advanced models for count data"
author: "Frank Edwards"
output: binb::metropolis
---

```{r setup, echo = FALSE, message = FALSE}
rm(list=ls())
library(tidyverse)
library(MASS)
library(rstanarm)
library(tidybayes)
select<-dplyr::select
set.seed(1)

options(xtable.comment = FALSE)

theme_set(theme_bw())
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
knitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE, size = "small")
```

## Count data

- Counts are cumulative totals of the number of incidences of some event, generally across time or place \pause
- Counts are positive integers $\in [0,\infty]$  \pause
- We can model count variables using the Poisson distribution

## The Poisson distribution (lambda = 1)

```{r, echo = F, fig.height = 3, size = "tiny"}
pois_1<-rpois(10000, 1)
table(pois_1)
qplot(pois_1)
```

## The Poisson distribution (lambda = 3)

```{r, echo = F, fig.height = 3, size = "tiny"}
pois_1<-rpois(10000, 3)
table(pois_1)
qplot(pois_1)
```

## The Poisson distribution (lambda = 7)

```{r, echo = F, fig.height = 3, size = "tiny"}
pois_1<-rpois(10000, 7)
table(pois_1)
qplot(pois_1)
```

## The Poisson distribution (lambda = 30)

```{r, echo = F, size = "tiny"}
pois_1<-rpois(10000, 30)
qplot(pois_1)
```

## The Poisson distribution (lambda = 150)

```{r, echo = F, size = "tiny"}
pois_1<-rpois(10000, 150)
qplot(pois_1)
```

## Special properties of the Poisson

- The variance and mean of a Poisson variable with parameter $\lambda$ are both equal to $\lambda$

\pause

```{r, size = "tiny"}
### draw a sample of 10,000 from a Poisson with lambda = 2.3
pois_demo<-rpois(10000, lambda = 2.3)
table(pois_demo)
```

\pause

```{r}
mean(pois_demo)
var(pois_demo)
```

## How well does a Poisson distribution fit our soccer data?

- Is the mean / variance assumption of a Poisson reasonable for our NWSL data on goal scoring?

```{r}
### Load in NWLS data
library(nwslR)
data("fieldplayer_overall_season_stats")
nwsl_stats<-fieldplayer_overall_season_stats 
### Check if mean == variance
mean(nwsl_stats$gls)
var(nwsl_stats$gls)
```

## Comparing distributions

```{r size = "tiny"}
### Draws from a Poisson with nrow() observations and lambda = mean(nwls$goals) 
sim1 <- rpois(nrow(nwsl_stats), lambda = mean(nwsl_stats$gls))
### simulation 
table(sim1)
### observed
table(nwsl_stats$gls)
```

## What's going on here?

- Problem 1: Player position is associated with goal scoring, right? Defenders don't score many (or any) goals. \pause

\pause

```{r}
nwsl_stats %>% 
  group_by(pos) %>% 
  summarize(gls_mn = mean(gls))
```

## So would simulating by position help yield a better fit?

```{r size = "tiny"}
### compute mean, variance, and N for each position
positions<-nwsl_stats %>% 
  group_by(pos) %>% 
  summarize(obs_mn = mean(gls),
            obs_var = var(gls),
            n_obs = n())

positions
```

## So would simulating by position help yield a better fit?

Does $E(goals|position) = var(goals|position)$?

```{r size = "tiny"}
### now simulate 10000 player - season totals by position
positions <- positions %>% 
  group_by(pos) %>% 
  mutate(sim_mn = mean(rpois(n_obs, obs_mn)),
         sim_var = var(rpois(n_obs, obs_mn)))

positions
```

## Overdispersion

Overdispersion is the presence of greater variability than we would expect from a given statistical model

\pause

For a Poisson model, we assume that 

$$var(x) = \bar{x} = \lambda$$

\pause

If $var(x) > \bar{x}$, then the data are *overdispersed* relative to predictions from the Poisson model.

## Clustering and overdispersion

When data come from distinct sub-populations, or clusters, they can have different underlying *data generating processes* (the real world process that produces our observations that we try to approximate using a statistical model). \pause

These differences in data generating processes often result in a) different expected values across sub-groups, and b) different levels of variability across sub-groups

## Poisson expectation

```{r echo = F}
positions<-nwsl_stats %>% 
  group_by(pos) %>% 
  summarize(obs_mn = mean(gls),
            obs_var = var(gls),
            n_obs = n())

nwsl_sim<-nwsl_stats %>% 
  left_join(positions) %>% 
  mutate(obs_n = 1:n()) %>% 
  group_by(obs_n) %>% 
  mutate(sim_gls = rpois(1, obs_mn))

ggplot(nwsl_sim,
       aes(x = sim_gls)) + 
  geom_histogram(fill = "blue", alpha = 0.5) + 
  facet_wrap(~pos, scales = "free") +
  labs(subtitle = "Blue = simulated from Poisson with lambda = position mean")
```

## Overdispersion

```{r echo = F}
ggplot(nwsl_sim,
       aes(x = sim_gls)) + 
  geom_histogram(fill = "blue", alpha = 0.5) + 
  geom_histogram(aes(x = gls), fill = "red", alpha = 0.5) +
  facet_wrap(~pos, scales = "free") +
  labs(subtitle = "Blue = simulated from Poisson, Red = observed")
```

## Overdispersion!

```{r}
nwsl_sim %>% 
  group_by(pos) %>% 
  summarize(obs_var = var(gls),
            sim_var = var(sim_gls))
```

## Modeling overdispersion: adding a shape parameter

We can relax the $var(x) = \bar{x}$ assumption of the Poisson likelihood with a *quasi-Poisson* likelihood that has the following properties:

$$E(x) = \lambda$$
$$var(x) = \theta \lambda$$

\pause

We call $\theta$ a dispersion or shape parameter. Higher values of $\theta$ result in more variability, lower values of $\theta$ result in more concentration.

## The Negative Binomial model

The negative binomial model is very similar to the quasi-poisson. It includes a mean parameter $\mu$ and a shape parameter $\theta$.

We can define a negative binomial likelihood as

$$x \sim \textrm{Negative Binomial}(\mu, \theta)$$

\pause

With an expected value

$$\bar{x} = \mu$$

and variance

$$var(x) = \mu + \frac{\mu^2}{\theta}$$

## Let's see if these likelihoods generate different results

```{r size = "tiny"}
options(mc.cores = parallel::detectCores(logical = FALSE))
goals_poisson<-stan_glm(gls ~ pos,
                        family = "poisson", 
                        data = nwsl_stats, 
                        refresh = 0)

goals_negbin<-stan_glm(gls ~ pos, 
                       family = "neg_binomial_2",
                       data = nwsl_stats,
                       refresh = 0)
```

## What do we notice about the results?

```{r size = "tiny"}
goals_poisson
goals_negbin
```

## Posterior distributions of Beta for forwards: setting up to plot

```{r size = "tiny"}
p_post<-data.frame(goals_poisson)%>% 
  mutate(model = "poisson")
n_post<-data.frame(goals_negbin) %>% 
  mutate(model = "negbin")

plot_dat<-bind_rows(p_post, n_post)
head(plot_dat)
```

## Posterior distributions of Beta: Forwards

```{r size = "tiny"}
ggplot(plot_dat,
       aes(x = posFW, y = model)) + 
  stat_halfeye()
```

## Posterior distributions of Beta: Midfielders

```{r size = "tiny"}
ggplot(plot_dat,
       aes(x = posMF, y = model)) + 
  stat_halfeye()
```

## Overdispersion and count models

Poisson likelihoods nearly always underestimate standard errors in complex social processes (especially under clustering). 

Our models *must* account for overdispersion if we want reasonable uncertainty estimates (standard errors, t-tests, prediction error, etc).

Negative binomial handles this problem well. Other approaches can work too!

## Let's evaluate predictive performance against the observed: Compute 90 percent predictive posterior intervals and empirical interval for goals

```{r size = "tiny"}
pos<-data.frame(pos = unique(nwsl_stats$pos))
pos_pois<-pos %>% 
  add_predicted_draws(goals_poisson) %>% 
  summarize(mn = mean(.prediction),
            upr = quantile(.prediction, 0.95),
            lwr = quantile(.prediction, 0.05)) %>% 
  mutate(model = "Poisson")

pos_negbin<-pos %>% 
  add_predicted_draws(goals_negbin) %>% 
  summarize(mn = mean(.prediction),
            upr = quantile(.prediction, 0.95),
            lwr = quantile(.prediction, 0.05)) %>% 
  mutate(model = "Negative Binomial")

pos_obs<-nwsl_stats %>% 
  group_by(pos) %>% 
  summarize(mn = mean(gls),
            upr = quantile(gls, 0.95),
            lwr = quantile(gls, 0.05)) %>% 
  mutate(model = "Observed")
  
plot_dat<-bind_rows(pos_pois, pos_negbin, pos_obs)
```

## Let's visualize the difference in model predictions

```{r echo = F}
ggplot(plot_dat,
       aes(x = model, 
           y = mn,
           ymin = lwr,
           ymax = upr)) + 
  geom_pointrange(size = 0.2) +
  facet_wrap(~pos, scales = "free") 
```

# Offsets in event count models 

## Offsets can improve model fit

Goals are a function of position, sure, but also a function of how many games a player appeared in.

\pause

An *offset* term can be added to our model to convert our count into a rate. 

\pause

Here, we can add `mp` as a measure of time

## The model for the mean

Using matches played as our offset variable

$$\textrm{goals} \sim \textrm{NegBin}(\mu, \theta)$$

$$\log(\mu) = \beta \times \textrm{position} + \log(\textrm{mp})$$

## A generic form

$$\log(\mu) = \beta X + \log(\textrm{offset})$$
\pause

Because $log(x) - log(y) = log(x/y)$ This can be rewritten as

$$\log \left( \frac{\mu}{\textrm{offset}} \right) = \beta X$$
\pause

That's a rate! With the inverse of the link function ($e$ here), we can write this as 

$$\frac{\mu}{\textrm{offset}} = e^{\beta X}$$

## Let's fit the model again

```{r size = "tiny", tidy = T}
goals_negbin_offset<-stan_glm(gls ~ pos,
                       data = nwsl_stats,
                       offset = log(mp),
                       family = "neg_binomial_2",
                       refresh = 0)

```

## Now we can compare model fits

```{r size = "tiny"}
loo_compare(loo(goals_negbin), loo(goals_negbin_offset))
```

The offset dramatically improves our model fit. Let's see how this works.

## The regression parameters

Let's compute the estimated number of goals under each model for a forward

$$\log(\textrm{goals}) = \beta_0 + \beta_1 \times \textrm{position}$$

```{r size = "tiny", tidy = T}
preds_1<-add_epred_draws(goals_negbin, 
                         newdata = data.frame(pos = "FW"))
```

## The regression parameters: offset model

$$E \left[\log(\frac{\textrm{goals}}{\textrm{games}})\right] = \beta_0 + \beta_1 \times \textrm{position}$$

```{r size = "tiny"}
preds_2<-add_epred_draws(goals_negbin_offset, 
                         newdata = data.frame(pos = rep("FW", 3),
                                              mp = c(1,10,20)),
                         offset = log(c(1, 10, 20)))
```

## And visualize the posterior expected values

```{r size = "tiny", fig.height = 5}
ggplot(preds_2,
       aes(x = .epred)) + 
  stat_halfeye() + 
  facet_wrap(~mp, 
             scales = "free")
```

## The difference between an offset and a predictor

```{r size = "tiny"}
goals_negbin2<-stan_glm(gls ~ pos + mp,
                        data = nwsl_stats,
                        family = "neg_binomial_2",
                        refresh = 0)

coef(goals_negbin_offset)
coef(goals_negbin2)
```

## Compare fits

```{r}
loo_compare(loo(goals_negbin2), 
            loo(goals_negbin_offset),
            loo(goals_negbin))
```

